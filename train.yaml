algorithm: PPO
timesteps:
  train_for: 100_000_000
  save_every: 50_000

parallelism: 6

random_seed: 0
net_arch:
  # default
  - pi: [64, 64]
    vf: [64, 64]
  
  # customised
  # [64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
